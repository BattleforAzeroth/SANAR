# SANAR部分
## 安装配置环境
1. 准备好虚拟环境，在项目目录下打开root终端（或root PyCharm）
2. 先安装PyTorch
3. 可能出现的问题
安装pip 24.0
```
python -m pip install pip==24.0
```

如缺少C++：
[Microsoft C++ 生成工具 - Visual Studio](https://visualstudio.microsoft.com/zh-hans/visual-cpp-build-tools/)

4. 不要直接运行setup.py，用命令：
```
pip install --editable ./
```
## 下载数据集
[GitHub Java 语料库](https://groups.inf.ed.ac.uk/cup/javaGithub/)
[150k Python Dataset | SRI Lab](https://www.sri.inf.ethz.ch/py150)
## 生成训练数据
配置位置
python2代码转3
train.txt文件中?替换_
# 预处理及Pyverilog部分

## Pyverilog安装
```
python3 setup.py install
```
注意安装Icarus Verilog:
```
sudo apt install iverilog
```

```
python fairseq_cli/train.py --source-lang xtoken --target-lang ytype --nwordstgt 50000 --nwordssrc 50000  --trainpref D:\source\PycharmProjects\SANAR\data_processing\nat_py_data\token_and_type\train --testpref D:\source\PycharmProjects\SANAR\data_processing\nat_py_data\token_and_type\eval --destdir D:\source\PycharmProjects\SANAR\data_processing\nat_py_data\bin --workers 1
```

--source-lang xtoken --target-lang ytoken --nwordstgt 50000 --nwordssrc 50000  --trainpref D:\source\PycharmProjects\SANAR\data_processing\nat_py_data\token_and_type\train --testpref D:\source\PycharmProjects\SANAR\data_processing\nat_py_data\token_and_type\eval --validpref D:\source\PycharmProjects\SANAR\data_processing\nat_py_data\token_and_type\eval --destdir D:\source\PycharmProjects\SANAR\data_processing\nat_py_data\bin2 --workers 1 --joined-dictionary

fairseq_cli/preprocess.py:
```
--source-lang xtoken --target-lang ytype --nwordstgt 50000 --nwordssrc 50000  --trainpref D:\source\PycharmProjects\SANAR\data_processing\verilog\token_and_type\train --testpref D:\source\PycharmProjects\SANAR\data_processing\verilog\token_and_type\eval --destdir D:\source\PycharmProjects\SANAR\data_processing\verilog\bin --workers 1
```
```
--source-lang xtoken --target-lang ytype --nwordstgt 50000 --nwordssrc 50000  --trainpref ${input_dir}/train --testpref ${input_dir}/eval --destdir ${save_dir} --workers 60
```
```
--source-lang xtoken --target-lang ytoken --nwordstgt 50000 --nwordssrc 50000  --trainpref D:\source\PycharmProjects\SANAR\data_processing\verilog\token_and_type\train --testpref D:\source\PycharmProjects\SANAR\data_processing\verilog\token_and_type\test --validpref D:\source\PycharmProjects\SANAR\data_processing\verilog\token_and_type\eval --destdir D:\source\PycharmProjects\SANAR\data_processing\verilog\bin2 --workers 1 --joined-dictionary
```
In H20:
python fairseq_cli/preprocess.py --source-lang xtoken --target-lang ytype --nwordstgt 50000 --nwordssrc 50000  --trainpref data_processing/verilog/token_and_type/train --testpref data_processing/verilog/token_and_type/test --validpref data_processing/verilog/token_and_type/eval --destdir data_processing/verilog/bin --workers 8
python fairseq_cli/preprocess.py --source-lang xtoken --target-lang ytoken --nwordstgt 50000 --nwordssrc 50000  --trainpref data_processing/verilog/token_and_type/train --testpref data_processing/verilog/token_and_type/test --validpref data_processing/verilog/token_and_type/eval --destdir data_processing/verilog/bin2 --workers 8 --joined-dictionary
python fairseq_cli/preprocess.py --source-lang xtoken --target-lang ytype --nwordstgt 50000 --nwordssrc 50000  --trainpref data_processing/verilog/token_and_type/train --testpref data_processing/verilog/token_and_type/test --destdir data_processing/verilog/bin --workers 8
python fairseq_cli/preprocess.py --source-lang xtoken --target-lang ytoken --nwordstgt 50000 --nwordssrc 50000  --trainpref data_processing/verilog/token_and_type/train --testpref data_processing/verilog/token_and_type/test --validpref data_processing/verilog/token_and_type/test --destdir data_processing/verilog/bin2 --workers 8 --joined-dictionary

# Train

D:\source\PycharmProjects\SANAR\data_processing\nat_py_data\traindata --user-dir sanar_plugins --noise full_mask --share-all-embeddings --source-lang xtoken --target-lang ytoken --label-smoothing 0.1 --lr 5e-5 --warmup-init-lr 1e-7 --stop-min-lr 1e-9 --lr-scheduler inverse_sqrt --warmup-updates 4000 --optimizer adam --adam-betas (0.9,0.999) --adam-eps 1e-6 --task translation_lev_modified --max-tokens 16384 --weight-decay 0.01 --dropout 0.1 --encoder-layers 6 --encoder-embed-dim 512 --decoder-layers 6 --decoder-embed-dim 512 --fp16 --max-source-positions 1000 --max-target-positions 1000 --max-epoch 30 --seed 0 --clip-norm 5 --save-dir D:\source\PycharmProjects\SANAR\pyck --src-embedding-copy --length-loss-factor 0.05 --log-interval 1000 --eval-bleu --eval-bleu-args {\"iter_decode_max_iter\":0,\"iter_decode_with_beam\":1} --eval-bleu-remove-bpe --best-checkpoint-metric bleu --maximize-best-checkpoint-metric --decoder-learned-pos --encoder-learned-pos --arch glat --criterion glat_loss --activation-fn gelu --valid-subset valid --skip-invalid-size-inputs-valid-test --max-tokens-valid 4096


 ```
D:\source\PycharmProjects\SANAR\data_processing\verilog\traindata --user-dir sanar_plugins --noise full_mask --share-all-embeddings --source-lang xtoken --target-lang ytoken --label-smoothing 0.1 --lr 5e-5 --warmup-init-lr 1e-7 --stop-min-lr 1e-9 --lr-scheduler inverse_sqrt --warmup-updates 4000 --optimizer adam --adam-betas (0.9,0.999) --adam-eps 1e-6 --task translation_lev_modified --max-tokens 16384 --weight-decay 0.01 --dropout 0.1 --encoder-layers 6 --encoder-embed-dim 512 --decoder-layers 6 --decoder-embed-dim 512 --fp16 --max-source-positions 1000 --max-target-positions 1000 --max-epoch 30 --seed 0 --clip-norm 5 --save-dir D:\source\PycharmProjects\SANAR\checkpoint --src-embedding-copy --length-loss-factor 0.05 --log-interval 1000 --eval-bleu --eval-bleu-args {\"iter_decode_max_iter\":0,\"iter_decode_with_beam\":1} --eval-bleu-remove-bpe --best-checkpoint-metric bleu --maximize-best-checkpoint-metric --decoder-learned-pos --encoder-learned-pos --arch glat --criterion glat_loss --activation-fn gelu --valid-subset valid --skip-invalid-size-inputs-valid-test --max-tokens-valid 4096
```

 ```
/mnt/tenant-home_speed/niyuting/SANAR/data_processing/verilog/traindata --user-dir sanar_plugins --noise full_mask --share-all-embeddings --source-lang xtoken --target-lang ytoken --label-smoothing 0.1 --lr 5e-5 --warmup-init-lr 1e-7 --stop-min-lr 1e-9 --lr-scheduler inverse_sqrt --warmup-updates 4000 --optimizer adam --adam-betas (0.9,0.999) --adam-eps 1e-6 --task translation_lev_modified --max-tokens 16384 --weight-decay 0.01 --dropout 0.1 --encoder-layers 6 --encoder-embed-dim 512 --decoder-layers 6 --decoder-embed-dim 512 --fp16 --max-source-positions 1000 --max-target-positions 1000 --max-epoch 30 --seed 0 --clip-norm 5 --save-dir /mnt/tenant-home_speed/niyuting/SANAR/checkpoint --src-embedding-copy --length-loss-factor 0.05 --log-interval 1000 --eval-bleu --eval-bleu-args {\"iter_decode_max_iter\":0,\"iter_decode_with_beam\":1} --eval-bleu-remove-bpe --best-checkpoint-metric bleu --maximize-best-checkpoint-metric --decoder-learned-pos --encoder-learned-pos --arch glat --criterion glat_loss --activation-fn gelu --valid-subset valid --skip-invalid-size-inputs-valid-test --max-tokens-valid 4096
```

 ```
CUDA_VISIBLE_DEVICES=4,5,6,7 python fairseq_cli/train.py /mnt/tenant-home_speed/niyuting/SANAR/data_processing/verilog/traindata --user-dir sanar_plugins --noise full_mask --share-all-embeddings --source-lang xtoken --target-lang ytoken --label-smoothing 0.1 --lr 5e-5 --warmup-init-lr 1e-7 --stop-min-lr 1e-9 --lr-scheduler inverse_sqrt --warmup-updates 4000 --optimizer adam --adam-betas '(0.9,0.999)' --adam-eps 1e-6 --task translation_lev_modified --max-tokens 16384 --weight-decay 0.01 --dropout 0.1 --encoder-layers 6 --encoder-embed-dim 512 --decoder-layers 6 --decoder-embed-dim 512 --fp16 --max-source-positions 1000 --max-target-positions 1000 --max-epoch 30 --seed 0 --clip-norm 5 --save-dir /mnt/tenant-home_speed/niyuting/SANAR/ckpts --src-embedding-copy --length-loss-factor 0.05 --log-interval 1000 --eval-bleu --eval-bleu-args '{"iter_decode_max_iter":0,"iter_decode_with_beam":1}' --eval-bleu-remove-bpe --best-checkpoint-metric bleu --maximize-best-checkpoint-metric --decoder-learned-pos --encoder-learned-pos --arch glat --criterion glat_loss --activation-fn gelu --valid-subset valid --skip-invalid-size-inputs-valid-test --max-tokens-valid 4096
```

CUDA_VISIBLE_DEVICES=4,5,6,7 python fairseq_cli/train.py /mnt/tenant-home_speed/niyuting/SANAR/data_processing/verilog/traindata \
--user-dir sanar_plugins --noise full_mask --share-all-embeddings \
--source-lang xtoken --target-lang ytoken \
--label-smoothing 0.1 --lr 5e-5 \
--warmup-init-lr 1e-7 --stop-min-lr 1e-9 \
--lr-scheduler inverse_sqrt --warmup-updates 4000 --optimizer adam \
--adam-betas '(0.9, 0.999)' \
--adam-eps 1e-6 --task translation_lev_modified --max-tokens 16384 \
--weight-decay 0.01 --dropout 0.1 \
--encoder-layers 6 --encoder-embed-dim 512 --decoder-layers 6 \
--decoder-embed-dim 512 --fp16 \
--max-source-positions 1000 --max-target-positions 1000 \
--max-epoch 30 --seed 0 --clip-norm 5 \
--save-dir /mnt/tenant-home_speed/niyuting/SANAR/checkpoint \
--src-embedding-copy --length-loss-factor 0.05 --log-interval 1000 \
--eval-bleu --eval-bleu-args '{"iter_decode_max_iter": 0, "iter_decode_with_beam": 1}' \
--eval-bleu-remove-bpe --best-checkpoint-metric bleu \
--maximize-best-checkpoint-metric --decoder-learned-pos --encoder-learned-pos \
--arch glat --criterion glat_loss \
--activation-fn gelu \
--valid-subset valid \
--skip-invalid-size-inputs-valid-test --max-tokens-valid 4096


```
pip uninstall numpy
pip install numpy==1.23.0

omegaconf==2.2.0
hydra-core==1.0.2
```

![[幻灯片1.png]]
# inference
```
D:\source\PycharmProjects\SANAR\data_processing\verilog\traindata --path D:\source\PycharmProjects\SANAR\checkpoint\checkpoint_best.pt --user-dir sanar_plugins --task translation_lev_modified --remove-bpe --max-tokens 2048 --source-lang xtoken --target-lang ytoken --iter-decode-max-iter 0 --iter-decode-eos-penalty 0 --iter-decode-with-beam 1 --gen-subset test --skip-invalid-size-inputs-valid-test --quiet
```

D:\source\PycharmProjects\SANAR\data_processing\nat_py_data\traindata --path D:\source\PycharmProjects\SANAR\checkpoint11.pt --user-dir sanar_plugins --task translation_lev_modified --remove-bpe --max-tokens 2048 --source-lang xtoken --target-lang ytoken --iter-decode-max-iter 0 --iter-decode-eos-penalty 0 --iter-decode-with-beam 1 --gen-subset test --skip-invalid-size-inputs-valid-test --quiet

In H20:
```
CUDA_VISIBLE_DEVICES=4,5,6,7 python fairseq_cli/generate.py ./data_processing/verilog/traindata --path ./checkpoint/checkpoint_best.pt --user-dir sanar_plugins --task translation_lev_modified --remove-bpe --max-tokens 2048 --source-lang xtoken --target-lang ytoken --iter-decode-max-iter 0 --iter-decode-eos-penalty 0 --iter-decode-with-beam 1 --gen-subset test --skip-invalid-size-inputs-valid-test --quiet
```


2025-05-20 01:56:46 | WARNING | fairseq.tasks.fairseq_task | 1,708 samples have invalid sizes and will be skipped, max_positions=(1000, 1000), first few sample ids=[2360027, 1645515, 366587, 3564166, 1527868, 1152841, 1374124, 3457147, 212801, 1034035]
2025-05-20 06:23:05 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.702 | nll_loss 2.843 | word_ins 4.496 | length 4.118 | ppl 26.03 | bleu 17.63 | wps 2558.3 | wpb 441.7 | bsz 39.8 | num_updates 43747
2025-05-20 06:23:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 43747 updates


2025-07-17 23:25:42 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/tenant-home_speed/niyuting/SANAR/checkpoint/checkpoint30.pt
2025-07-17 23:25:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /mnt/tenant-home_speed/niyuting/SANAR/checkpoint/checkpoint30.pt (epoch 30 @ 342975 updas, score 58.31) (writing took 13.954570560250431 seconds)
2025-07-17 23:25:54 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2025-07-17 23:25:54 | INFO | train | epoch 030 | loss 2.547 | nll_loss 0.629 | glat_accu 0.921 | glat_context_p 0.3 | word_ins 2.487 | length 1.236 | ppl 5.85 | s 67902.4 | ups 5.83 | wpb 11640.8 | bsz 1176.6 | num_updates 342975 | lr 5.39969e-06 | gnorm 0.789 | clip 0 | loss_scale 32768 | train_wall 1141 | gb_free 90 | ll 70146
2025-07-17 23:25:54 | INFO | fairseq_cli.train | done training in 70137.0 seconds
/mnt/tenant-home_speed/niyuting/SANAR/.venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `it_process_group` or `barrier `. Using the current device set by the user.


![[Pasted image 20250525115733.png]]


![[Pasted image 20250526182323.png]]

2025-07-18 01:09:07 | WARNING | fairseq.tasks.fairseq_task | 340 samples have invalid sizes and will be skipped, max_positions=(1000, 1000), first few sample ids=[1786109, 34005, 34050, 1496728, 51654, 711028, 711026, 711045, 711022, 711050]
2025-07-18 04:15:49 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2
2025-07-18 04:15:49 | INFO | fairseq_cli.generate | Translated 1,832,856 sentences (17,784,809 tokens) in 1241.6s (1476.22 sentences/s, 14324.21 tokens/s)
2025-07-18 04:18:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2025-07-18 04:18:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2025-07-18 04:18:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
Generate test with beam=5: BLEU: 69.4894, EDIT_SIM = 0.8755, ITEM_ACC = 0.5685, ACC = 0.5639, following ACC = 0.5856
